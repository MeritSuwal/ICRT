{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:04.484961Z","iopub.status.busy":"2025-03-09T11:05:04.484571Z","iopub.status.idle":"2025-03-09T11:05:04.490972Z","shell.execute_reply":"2025-03-09T11:05:04.490083Z","shell.execute_reply.started":"2025-03-09T11:05:04.484927Z"},"trusted":true},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","import h5py\n","import json\n","import torch\n","from tqdm import tqdm\n","from collections import Counter, defaultdict\n","from random import seed, choice, sample\n","import imageio\n","from PIL import Image \n","\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","#BLEU\n","import copy\n","import sys, math, re\n","\n","#DATASET\n","from torch.utils.data import Dataset\n","\n","#MODELS (CNN+TF)\n","from torch import nn\n","import torchvision\n","\n","#TRAIN\n","import torch.backends.cudnn as cudnn\n","import torch.optim\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","from nltk.translate.bleu_score import corpus_bleu\n","import codecs\n","\n","#EVALUATE\n","import torch.nn.functional as F\n","\n","# CAPTIONING\n","#import imageio.v2 as imageio\n","import matplotlib.cm as cm\n","import skimage.transform\n","# from scipy.misc import imread, imresize\n","# import transformer, models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:07.116891Z","iopub.status.busy":"2025-03-09T11:05:07.116547Z","iopub.status.idle":"2025-03-09T11:05:07.120901Z","shell.execute_reply":"2025-03-09T11:05:07.120036Z","shell.execute_reply.started":"2025-03-09T11:05:07.116864Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:07.997065Z","iopub.status.busy":"2025-03-09T11:05:07.996715Z","iopub.status.idle":"2025-03-09T11:05:08.001870Z","shell.execute_reply":"2025-03-09T11:05:08.000851Z","shell.execute_reply.started":"2025-03-09T11:05:07.997042Z"},"trusted":true},"outputs":[],"source":["print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# ARGUMENTS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:09.676286Z","iopub.status.busy":"2025-03-09T11:05:09.676000Z","iopub.status.idle":"2025-03-09T11:05:09.685526Z","shell.execute_reply":"2025-03-09T11:05:09.684639Z","shell.execute_reply.started":"2025-03-09T11:05:09.676266Z"},"trusted":true},"outputs":[],"source":["#create_ipfiles\n","dataset = \"coco\"\n","karpathy_json_path = \"/kaggle/input/icrt-coco/dataset/caption_dataset/dataset_coco.json\"\n","image_folder = \"/kaggle/input/icrt-coco/dataset/\"\n","captions_per_image = 5\n","min_word_freq = 5 #the minimum frequency of words\n","output_folder = '/kaggle/working/' #output filepath\n","max_len = 50 #the maximum length of each caption\n","\n","data_folder = \"/kaggle/input/icrt-coco/dataset/generated_data\" #folder with data files saved by create_input_files.py\n","data_name =\"coco_5_cap_per_img_5_min_word_freq\" #base name shared by data files\n","\n","# Model parameters\n","emb_dim = 300 #dimension of word embedding\n","attention_dim = 512 #dimension of attention linear layers\n","decoder_dim =512 #dimension of decoder RNN\n","n_heads = 8 # Multi-head attention\n","dropout = 0.2\n","encoder_mode = '_rn101_'\n","decoder_mode = \"transformer\"\n","encoder_layers = 2  #the number of layers of encoder in Transformer\n","decoder_layers = 4 # the number of layers of decoder in Transformer\n","\n","# Training parameters\n","epochs = 1\n","stop_criteria = 25 # training stop if epochs_since_improvement == stop_criteria\n","batch_size = 32\n","print_freq = 100 # print training/validation stats every __ batches\n","workers = 1 # for data-loading; right now, only 1 works with h5pys\n","encoder_lr = 1e-4 # learning rate for encoder if fine-tuning\n","decoder_lr = 1e-4 #learning rate for decoder\n","grad_clip = 5. #clip gradients at an absolute value of\n","alpha_c = 1. # regularization parameter for doubly stochastic attention, as in the paper\n","weight_decay = 1e-2\n","\n","fine_tune_encoder = True #'whether fine-tune encoder or not\n","fine_tune_embedding = False\n","checkpoint = None\n","losses_path = None\n","embedding_path = '/kaggle/input/icrt-coco/dataset/Glove/glove.6B.300d.txt' #path to pre-trained word Embedding or None"]},{"cell_type":"markdown","metadata":{},"source":["# Creating Input Files"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:12.708827Z","iopub.status.busy":"2025-03-09T11:05:12.708468Z","iopub.status.idle":"2025-03-09T11:05:12.724398Z","shell.execute_reply":"2025-03-09T11:05:12.723555Z","shell.execute_reply.started":"2025-03-09T11:05:12.708798Z"},"trusted":true},"outputs":[],"source":["def create_input_files(dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,\n","                       max_len=100):\n","    \"\"\"\n","    Creates input files for training, validation, and test data.\n","\n","    Arguments:\n","        dataset: name of dataset, one of 'coco', 'flickr8k', 'flickr30k'\n","        karpathy_json_path: path of Karpathy JSON file with splits and captions\n","        image_folder: folder with downloaded images\n","        captions_per_image: number of captions to sample per image\n","        min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n","        output_folder: folder to save files\n","        max_len: don't sample captions longer than this length\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    assert dataset in {'coco'}\n","\n","    # Read Karpathy JSON\n","    with open(karpathy_json_path, 'r') as j:\n","        data = json.load(j)\n","\n","    # Read image paths and captions for each image\n","    train_image_paths = []\n","    train_image_captions = []\n","    val_image_paths = []\n","    val_image_captions = []\n","    test_image_paths = []\n","    test_image_captions = []\n","    word_freq = Counter()\n","\n","    for img in data['images']:\n","        captions = []\n","        for c in img['sentences']:\n","            # Update word frequency\n","            word_freq.update(c['tokens'])\n","            if len(c['tokens']) <= max_len:\n","                captions.append(c['tokens'])  # [[0], [1], [2], [3], [4]]\n","\n","        if len(captions) == 0:\n","            continue\n","\n","        path = os.path.join(image_folder, img['filepath'], img['filename']) \n","\n","        if img['split'] in {'train', 'restval'}:\n","            train_image_paths.append(path)\n","            train_image_captions.append(captions)\n","        elif img['split'] in {'val'}:\n","            val_image_paths.append(path)\n","            val_image_captions.append(captions)\n","        elif img['split'] in {'test'}:\n","            test_image_paths.append(path)\n","            test_image_captions.append(captions)\n","\n","    # Sanity check\n","    assert len(train_image_paths) == len(train_image_captions)\n","    assert len(val_image_paths) == len(val_image_captions)\n","    assert len(test_image_paths) == len(test_image_captions)\n","    print(\"find {} training data, {} val data, {} test data\".format(len(train_image_paths), len(val_image_paths), len(test_image_paths)))\n","\n","    # Create word map\n","    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","    word_map = {k: v + 1 for v, k in enumerate(words)}  # word2id\n","    word_map['<unk>'] = len(word_map) + 1\n","    word_map['<start>'] = len(word_map) + 1\n","    word_map['<end>'] = len(word_map) + 1\n","    word_map['<pad>'] = 0\n","\n","    # Create a base/root name for all output files\n","    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n","\n","    # Save word map to a JSON\n","    with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n","        json.dump(word_map, j)\n","    print(\"{} words write into WORDMAP\".format(len(word_map)))\n","\n","    # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n","    seed(123)\n","    for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n","                                   (val_image_paths, val_image_captions, 'VAL'),\n","                                   (test_image_paths, test_image_captions, 'TEST')]:\n","\n","        with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n","            # Make a note of the number of captions we are sampling per image\n","            h.attrs['captions_per_image'] = captions_per_image\n","\n","            # Create dataset inside HDF5 file to store images\n","            images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n","\n","            print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n","\n","            enc_captions = []\n","            caplens = []\n","\n","            for i, path in enumerate(tqdm(impaths)):\n","\n","                # Sample captions\n","                if len(imcaps[i]) < captions_per_image:\n","                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n","                else:\n","                    captions = sample(imcaps[i], k=captions_per_image)\n","\n","                # Sanity check\n","                assert len(captions) == captions_per_image\n","\n","                # Read images\n","                img = imageio.imread(impaths[i])\n","                # img = imread(impaths[i])\n","                if len(img.shape) == 2:\n","                    # gray-scale\n","                    img = img[:, :, np.newaxis]\n","                    img = np.concatenate([img, img, img], axis=2)  # [256, 256, 1+1+1]\n","                img = np.array(Image.fromarray(img).resize((256, 256)))\n","                # img = imresize(img, (256, 256))\n","                img = img.transpose(2, 0, 1)\n","                assert img.shape == (3, 256, 256)\n","                assert np.max(img) <= 255\n","\n","                # Save image to HDF5 file\n","                images[i] = img\n","\n","                for j, c in enumerate(captions):\n","                    # Encode captions\n","                    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in c] + [\n","                        word_map['<end>']] + [word_map['<pad>']] * (max_len - len(c))\n","\n","                    # Find caption lengths\n","                    c_len = len(c) + 2\n","\n","                    enc_captions.append(enc_c)\n","                    print(enc_captions)\n","                    caplens.append(c_len)\n","                    print(c_len)\n","\n","            # Sanity check\n","            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n","\n","            # Save encoded captions and their lengths to JSON files\n","            with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n","                json.dump(enc_captions, j)\n","\n","            with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n","                json.dump(caplens, j)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:15.298904Z","iopub.status.busy":"2025-03-09T11:05:15.298543Z","iopub.status.idle":"2025-03-09T11:05:15.307127Z","shell.execute_reply":"2025-03-09T11:05:15.306045Z","shell.execute_reply.started":"2025-03-09T11:05:15.298878Z"},"trusted":true},"outputs":[],"source":["class CaptionDataset(Dataset):\n","    \"\"\"\n","    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n","    \n","    \"\"\"\n","\n","    def __init__(self, data_folder, data_name, split, transform=None):\n","        \"\"\"\n","        Arguments:\n","            data_folder: folder where data files are stored - /Users/skye/docs/image_dataset/dataset\n","            data_name: base name of processed datasets\n","            split: split, one of 'TRAIN', 'VAL', or 'TEST'\n","            transform: image transform pipeline\n","        \"\"\"\n","        self.split = split\n","        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n","\n","        # Open hdf5 file where images are stored\n","        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n","        self.imgs = self.h['images']\n","\n","        # Captions per image\n","        self.cpi = self.h.attrs['captions_per_image']\n","\n","        # Load encoded captions (completely into memory)\n","        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n","            self.captions = json.load(j)\n","\n","        # Load caption lengths (completely into memory)\n","        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n","            self.caplens = json.load(j)\n","\n","        # PyTorch transformation pipeline for the image (normalizing, etc.)\n","        self.transform = transform\n","\n","        # Total number of datapoints\n","        self.dataset_size = len(self.captions)\n","\n","    def __getitem__(self, i):\n","        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n","        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        caption = torch.LongTensor(self.captions[i])\n","\n","        caplen = torch.LongTensor([self.caplens[i]])\n","\n","        if self.split == 'TRAIN':\n","            return img, caption, caplen\n","        else:\n","            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n","            all_captions = torch.LongTensor(self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n","            return img, caption, caplen, all_captions\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"markdown","metadata":{},"source":["# CNN Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:17.379897Z","iopub.status.busy":"2025-03-09T11:05:17.379545Z","iopub.status.idle":"2025-03-09T11:05:17.386465Z","shell.execute_reply":"2025-03-09T11:05:17.385541Z","shell.execute_reply.started":"2025-03-09T11:05:17.379869Z"},"trusted":true},"outputs":[],"source":["class CNN_Encoder(nn.Module):\n","    \"\"\"\n","    CNN_Encoder.\n","    \"\"\"\n","\n","    def __init__(self, encoded_image_size=14):\n","        super(CNN_Encoder, self).__init__()\n","        self.enc_image_size = encoded_image_size\n","\n","        resnet = torchvision.models.resnet101(weights='DEFAULT')  # pretrained ImageNet ResNet\n","\n","        # Remove linear and pool layers (since we're not doing classification)\n","        # Specifically, Remove: AdaptiveAvgPool2d(output_size=(1, 1)), Linear(in_features=2048, out_features=1000, bias=True)]\n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","\n","        # Resize image to fixed size to allow input images of variable size\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n","\n","        self.fine_tune()\n","\n","    def forward(self, images):\n","        \"\"\"\n","        Forward propagation.\n","\n","        Arguemnts:\n","            images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n","\n","        Returns:\n","            encoded images [batch_size, encoded_image_size=14, encoded_image_size=14, 2048]\n","        \"\"\"\n","        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n","        out = self.adaptive_pool(out)  # [batch_size, 2048/512, 8, 8] -> [batch_size, 2048/512, 14, 14]\n","        out = out.permute(0, 2, 3, 1)\n","        return out\n","\n","    def fine_tune(self, fine_tune=True):\n","        \"\"\"\n","        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n","\n","        Arguements:\n","            fine_tune: Allow?\n","        \"\"\"\n","        for p in self.resnet.parameters():\n","            p.requires_grad = False\n","        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n","        for c in list(self.resnet.children())[5:]:\n","            for p in c.parameters():\n","                p.requires_grad = fine_tune"]},{"cell_type":"markdown","metadata":{},"source":["# Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:18.811684Z","iopub.status.busy":"2025-03-09T11:05:18.811381Z","iopub.status.idle":"2025-03-09T11:05:18.815465Z","shell.execute_reply":"2025-03-09T11:05:18.814534Z","shell.execute_reply.started":"2025-03-09T11:05:18.811650Z"},"trusted":true},"outputs":[],"source":["channel_number = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:19.291016Z","iopub.status.busy":"2025-03-09T11:05:19.290697Z","iopub.status.idle":"2025-03-09T11:05:19.296233Z","shell.execute_reply":"2025-03-09T11:05:19.295349Z","shell.execute_reply.started":"2025-03-09T11:05:19.290995Z"},"trusted":true},"outputs":[],"source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, QKVdim):\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.QKVdim = QKVdim\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        \"\"\"\n","        Arguments:\n","            Q: [batch_size, n_heads, -1(len_q), QKVdim]\n","            K, V: [batch_size, n_heads, -1(len_k=len_v), QKVdim]\n","            attn_mask: [batch_size, n_heads, len_q, len_k]\n","        \"\"\"\n","        # scores: [batch_size, n_heads, len_q, len_k]\n","        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.QKVdim)\n","        # Fills elements of self tensor with value where mask is True.\n","        scores.to(device).masked_fill_(attn_mask, -1e9)\n","        attn = nn.Softmax(dim=-1)(scores)  # [batch_size, n_heads, len_q, len_k]\n","        context = torch.matmul(attn, V).to(device)  # [batch_size, n_heads, len_q, QKVdim]\n","        return context, attn"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:20.988273Z","iopub.status.busy":"2025-03-09T11:05:20.987972Z","iopub.status.idle":"2025-03-09T11:05:20.996329Z","shell.execute_reply":"2025-03-09T11:05:20.995471Z","shell.execute_reply.started":"2025-03-09T11:05:20.988251Z"},"trusted":true},"outputs":[],"source":["class Multi_Head_Attention(nn.Module):\n","    def __init__(self, Q_dim, K_dim, QKVdim, n_heads=8, dropout=0.1):\n","        super(Multi_Head_Attention, self).__init__()\n","        self.W_Q = nn.Linear(Q_dim, QKVdim * n_heads).to(device)\n","        self.W_K = nn.Linear(K_dim, QKVdim * n_heads).to(device)\n","        self.W_V = nn.Linear(K_dim, QKVdim * n_heads).to(device)\n","        self.n_heads = n_heads\n","        self.QKVdim = QKVdim\n","        self.embed_dim = Q_dim\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.W_O = nn.Linear(self.n_heads * self.QKVdim, self.embed_dim).to(device)\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        \"\"\"\n","        In self-encoder attention:\n","                Q = K = V: [batch_size, num_pixels=196, encoder_dim=2048]\n","                attn_mask: [batch_size, len_q=196, len_k=196]\n","        In self-decoder attention:\n","                Q = K = V: [batch_size, max_len=52, embed_dim=512]\n","                attn_mask: [batch_size, len_q=52, len_k=52]\n","        encoder-decoder attention:\n","                Q: [batch_size, 52, 512] from decoder\n","                K, V: [batch_size, 196, 2048] from encoder\n","                attn_mask: [batch_size, len_q=52, len_k=196]\n","        return _, attn: [batch_size, n_heads, len_q, len_k]\n","        \"\"\"\n","        residual, batch_size = Q, Q.size(0)\n","        # q_s: [batch_size, n_heads=8, len_q, QKVdim] k_s/v_s: [batch_size, n_heads=8, len_k, QKVdim]\n","        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n","        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n","        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.QKVdim).transpose(1, 2)\n","        # attn_mask: [batch_size, self.n_heads, len_q, len_k]\n","        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n","        # attn: [batch_size, n_heads, len_q, len_k]\n","        # context: [batch_size, n_heads, len_q, QKVdim]\n","        context, attn = ScaledDotProductAttention(self.QKVdim)(q_s, k_s, v_s, attn_mask)\n","        # context: [batch_size, n_heads, len_q, QKVdim] -> [batch_size, len_q, n_heads * QKVdim]\n","        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.QKVdim).to(device)\n","        # output: [batch_size, len_q, embed_dim]\n","        output = self.W_O(context)\n","        output = self.dropout(output)\n","        return nn.LayerNorm(self.embed_dim).to(device)(output + residual), attn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:23.155082Z","iopub.status.busy":"2025-03-09T11:05:23.154784Z","iopub.status.idle":"2025-03-09T11:05:23.161184Z","shell.execute_reply":"2025-03-09T11:05:23.160033Z","shell.execute_reply.started":"2025-03-09T11:05:23.155061Z"},"trusted":true},"outputs":[],"source":["class PoswiseFeedForwardNet(nn.Module):\n","    def __init__(self, embed_dim, d_ff, dropout):\n","        super(PoswiseFeedForwardNet, self).__init__()\n","        \"\"\"\n","        Two fc layers can also be described by two cnn with kernel_size=1.\n","        \"\"\"\n","        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=d_ff, kernel_size=1).to(device)\n","        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=embed_dim, kernel_size=1).to(device)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.embed_dim = embed_dim\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        encoder: inputs: [batch_size, len_q=196, embed_dim=2048]\n","        decoder: inputs: [batch_size, max_len=52, embed_dim=512]\n","        \"\"\"\n","        residual = inputs\n","        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n","        output = self.conv2(output).transpose(1, 2)\n","        output = self.dropout(output)\n","        return nn.LayerNorm(self.embed_dim).to(device)(output + residual)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:23.978688Z","iopub.status.busy":"2025-03-09T11:05:23.978374Z","iopub.status.idle":"2025-03-09T11:05:23.984571Z","shell.execute_reply":"2025-03-09T11:05:23.983477Z","shell.execute_reply.started":"2025-03-09T11:05:23.978664Z"},"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, embed_dim, dropout, n_heads):\n","        super(DecoderLayer, self).__init__()\n","        self.dec_self_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=embed_dim, QKVdim=64, n_heads=n_heads, dropout=dropout)\n","        \n","        self.dec_enc_attn = Multi_Head_Attention(Q_dim=embed_dim, K_dim=2048, QKVdim=64, n_heads=n_heads, dropout=dropout)\n","        self.pos_ffn = PoswiseFeedForwardNet(embed_dim=embed_dim, d_ff=2048, dropout=dropout)\n","        \n","    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n","        \"\"\"\n","        Arguments:\n","            dec_inputs: [batch_size, max_len=52, embed_dim=512]\n","            enc_outputs: [batch_size, num_pixels=196, 2048]\n","            dec_self_attn_mask: [batch_size, 52, 52]\n","            dec_enc_attn_mask: [batch_size, 52, 196]\n","        \"\"\"\n","        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)\n","        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n","        dec_outputs = self.pos_ffn(dec_outputs)\n","        return dec_outputs, dec_self_attn, dec_enc_attn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:25.144811Z","iopub.status.busy":"2025-03-09T11:05:25.144494Z","iopub.status.idle":"2025-03-09T11:05:25.156522Z","shell.execute_reply":"2025-03-09T11:05:25.155670Z","shell.execute_reply.started":"2025-03-09T11:05:25.144789Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, n_layers, vocab_size, embed_dim, dropout, n_heads):\n","        super(Decoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.tgt_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n","        self.pos_emb = nn.Embedding.from_pretrained(self.get_position_embedding_table(embed_dim), freeze=True)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.layers = nn.ModuleList([DecoderLayer(embed_dim, dropout, n_heads) for _ in range(n_layers)])\n","        self.projection = nn.Linear(embed_dim, vocab_size, bias=False).to(device)\n","\n","    def get_position_embedding_table(self, embed_dim):\n","        def cal_angle(position, hid_idx):\n","            return position / np.power(10000, 2 * (hid_idx // 2) / embed_dim)\n","        def get_posi_angle_vec(position):\n","            return [cal_angle(position, hid_idx) for hid_idx in range(embed_dim)]\n","\n","        embedding_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(52)])\n","        embedding_table[:, 0::2] = np.sin(embedding_table[:, 0::2])  # dim 2i\n","        embedding_table[:, 1::2] = np.cos(embedding_table[:, 1::2])  # dim 2i+1\n","        return torch.FloatTensor(embedding_table).to(device)\n","\n","    def get_attn_pad_mask(self, seq_q, seq_k):\n","        batch_size, len_q = seq_q.size()\n","        batch_size, len_k = seq_k.size()\n","        # In wordmap, <pad>:0\n","        # pad_attn_mask: [batch_size, 1, len_k], one is masking\n","        pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n","        return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]\n","\n","    def get_attn_subsequent_mask(self, seq):\n","        attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n","        subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n","        subsequent_mask = torch.from_numpy(subsequent_mask).byte().to(device)\n","        return subsequent_mask\n","\n","    def forward(self, encoder_out, encoded_captions, caption_lengths):\n","        \"\"\"\n","        Arguments:\n","            encoder_out: [batch_size, num_pixels=196, 2048]\n","            encoded_captions: [batch_size, 52]\n","            caption_lengths: [batch_size, 1]\n","        \"\"\"\n","        batch_size = encoder_out.size(0)\n","        # Sort input data by decreasing lengths.\n","        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n","        sort_ind = sort_ind.to(device)\n","        encoder_out = encoder_out[sort_ind]\n","        encoded_captions = encoded_captions[sort_ind]\n","        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n","        # So, decoding lengths are actual lengths - 1\n","        decode_lengths = (caption_lengths - 1).tolist()\n","\n","        # dec_outputs: [batch_size, max_len=52, embed_dim=512]\n","        # dec_self_attn_pad_mask: [batch_size, len_q=52, len_k=52], 1 if id=0(<pad>)\n","        # dec_self_attn_subsequent_mask: [batch_size, 52, 52], Upper triangle of an array with 1.\n","        # dec_self_attn_mask for self-decoder attention, the position whose val > 0 will be masked.\n","        # dec_enc_attn_mask for encoder-decoder attention.\n","        # e.g. 9488, 23, 53, 74, 0, 0  |  dec_self_attn_mask:\n","        # 0 1 1 1 2 2\n","        # 0 0 1 1 2 2\n","        # 0 0 0 1 2 2\n","        # 0 0 0 0 2 2\n","        # 0 0 0 0 1 2\n","        # 0 0 0 0 1 1\n","        dec_outputs = self.tgt_emb(encoded_captions) + self.pos_emb(torch.LongTensor([list(range(52))]*batch_size).to(device))\n","        dec_outputs = self.dropout(dec_outputs)\n","        dec_self_attn_pad_mask = self.get_attn_pad_mask(encoded_captions, encoded_captions)\n","        dec_self_attn_subsequent_mask = self.get_attn_subsequent_mask(encoded_captions)\n","        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n","        \n","        dec_enc_attn_mask = (torch.tensor(np.zeros((batch_size, 52, 196))).to(device) == torch.tensor(np.ones((batch_size, 52, 196))).to(device))\n","       \n","        dec_self_attns, dec_enc_attns = [], []\n","        for layer in self.layers:\n","            # attn: [batch_size, n_heads, len_q, len_k]\n","            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, encoder_out, dec_self_attn_mask, dec_enc_attn_mask)\n","            dec_self_attns.append(dec_self_attn)\n","            dec_enc_attns.append(dec_enc_attn)\n","        predictions = self.projection(dec_outputs)\n","        return predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:27.402600Z","iopub.status.busy":"2025-03-09T11:05:27.402290Z","iopub.status.idle":"2025-03-09T11:05:27.408328Z","shell.execute_reply":"2025-03-09T11:05:27.407314Z","shell.execute_reply.started":"2025-03-09T11:05:27.402576Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, dropout, n_heads):\n","        super(EncoderLayer, self).__init__()\n","        \"\"\"\n","        As per \"Attention is all you need\" paper, dk = dv = 64, h = 8, N=6\n","        \"\"\"\n","        \n","        self.enc_self_attn = Multi_Head_Attention(Q_dim=2048, K_dim=2048, QKVdim=64, n_heads=n_heads, dropout=dropout)\n","        self.pos_ffn = PoswiseFeedForwardNet(embed_dim=2048, d_ff=4096, dropout=dropout)\n","        \n","    def forward(self, enc_inputs, enc_self_attn_mask):\n","        \"\"\"\n","        Arguments:\n","            enc_inputs: [batch_size, num_pixels=196, 2048]\n","            enc_outputs: [batch_size, len_q=196, d_model=2048]\n","\n","        Returns:\n","            attn: [batch_size, n_heads=8, 196, 196]\n","        \"\"\"\n","        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n","        enc_outputs = self.pos_ffn(enc_outputs)\n","        return enc_outputs, attn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:41.419280Z","iopub.status.busy":"2025-03-09T11:05:41.418964Z","iopub.status.idle":"2025-03-09T11:05:41.427768Z","shell.execute_reply":"2025-03-09T11:05:41.426806Z","shell.execute_reply.started":"2025-03-09T11:05:41.419256Z"},"trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, n_layers, dropout, n_heads):\n","        super(Encoder, self).__init__()\n","        self.pos_emb = nn.Embedding.from_pretrained(self.get_position_embedding_table(), freeze=True)\n","        # self.dropout = nn.Dropout(p=dropout)\n","        self.layers = nn.ModuleList([EncoderLayer(dropout, n_heads) for _ in range(n_layers)])\n","\n","    def get_position_embedding_table(self):\n","        def cal_angle(position, hid_idx):\n","            x = position % 14\n","            y = position // 14\n","            x_enc = x / np.power(10000, hid_idx / 1024)\n","            y_enc = y / np.power(10000, hid_idx / 1024)\n","            return np.sin(x_enc), np.sin(y_enc)\n","        def get_posi_angle_vec(position):\n","            return [cal_angle(position, hid_idx)[0] for hid_idx in range(1024)] + [cal_angle(position, hid_idx)[1] for hid_idx in range(1024)]\n","\n","        embedding_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(196)])\n","        return torch.FloatTensor(embedding_table).to(device)\n","\n","    def forward(self, encoder_out):\n","        \"\"\"\n","        Arguments:\n","            encoder_out: [batch_size, num_pixels=196, dmodel=2048]\n","        \"\"\"\n","        batch_size = encoder_out.size(0)\n","        positions = encoder_out.size(1)\n","        \n","        encoder_out = encoder_out + self.pos_emb(torch.LongTensor([list(range(positions))]*batch_size).to(device))\n","        # encoder_out = self.dropout(encoder_out)\n","        # enc_self_attn_mask: [batch_size, 196, 196]\n","        enc_self_attn_mask = (torch.tensor(np.zeros((batch_size, positions, positions))).to(device)\n","                              == torch.tensor(np.ones((batch_size, positions, positions))).to(device))\n","        enc_self_attns = []\n","        for layer in self.layers:\n","            encoder_out, enc_self_attn = layer(encoder_out, enc_self_attn_mask)\n","            enc_self_attns.append(enc_self_attn)\n","        return encoder_out, enc_self_attns\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:42.420139Z","iopub.status.busy":"2025-03-09T11:05:42.419845Z","iopub.status.idle":"2025-03-09T11:05:42.427114Z","shell.execute_reply":"2025-03-09T11:05:42.426237Z","shell.execute_reply.started":"2025-03-09T11:05:42.420118Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    \"\"\"\n","    See paper 5.4: \"Attention Is All You Need\" - https://arxiv.org/abs/1706.03762\n","    \"Apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n","    In addition, apply dropout to the sums of the embeddings and the positional encodings in both the encoder\n","    and decoder stacks.\" (Now, we dont't apply dropout to the encoder embeddings)\n","    \"\"\"\n","    def __init__(self, vocab_size, embed_dim, encoder_layers, decoder_layers, dropout=0.1, n_heads=8):\n","        super(Transformer, self).__init__()\n","        self.encoder = Encoder(encoder_layers, dropout, n_heads)\n","        self.decoder = Decoder(decoder_layers, vocab_size, embed_dim, dropout, n_heads)\n","        self.embedding = self.decoder.tgt_emb\n","\n","    def load_pretrained_embeddings(self, embeddings):\n","        self.embedding.weight = nn.Parameter(embeddings)\n","\n","    def fine_tune_embeddings(self, fine_tune=True):\n","        for p in self.embedding.parameters():\n","            p.requires_grad = fine_tune\n","\n","    def forward(self, enc_inputs, encoded_captions, caption_lengths):\n","        \"\"\"\n","        preprocess: enc_inputs: [batch_size, 14, 14, 2048]/[batch_size, 196, 2048] -> [batch_size, 196, 2048]\n","        encoded_captions: [batch_size, 52]\n","        caption_lengths: [batch_size, 1], not used\n","        The encoder or decoder is composed of a stack of n_layers=6 identical layers.\n","        One layer in encoder: Multi-head Attention(self-encoder attention) with Norm & Residual\n","                            + Feed Forward with Norm & Residual\n","        One layer in decoder: Masked Multi-head Attention(self-decoder attention) with Norm & Residual\n","                            + Multi-head Attention(encoder-decoder attention) with Norm & Residual\n","                            + Feed Forward with Norm & Residual\n","        \"\"\"\n","        batch_size = enc_inputs.size(0)\n","        encoder_dim = enc_inputs.size(-1)\n","        \n","        enc_inputs = enc_inputs.view(batch_size, -1, encoder_dim)\n","        \n","        encoder_out, enc_self_attns = self.encoder(enc_inputs)\n","        # encoder_out: [batch_size, 196, 2048]\n","        predictions, encoded_captions, decode_lengths, sort_ind, dec_self_attns, dec_enc_attns = self.decoder(encoder_out, encoded_captions, caption_lengths)\n","        alphas = {\"enc_self_attns\": enc_self_attns, \"dec_self_attns\": dec_self_attns, \"dec_enc_attns\": dec_enc_attns}\n","        predictions.shape\n","        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# HELPER FUNCTIONS FOR TRAINING AND VALIDATING THE MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:45.211257Z","iopub.status.busy":"2025-03-09T11:05:45.210933Z","iopub.status.idle":"2025-03-09T11:05:45.215780Z","shell.execute_reply":"2025-03-09T11:05:45.214859Z","shell.execute_reply.started":"2025-03-09T11:05:45.211231Z"},"trusted":true},"outputs":[],"source":["# Save losses to an HDF5 file\n","def save_losses_h5(training_losses, validation_losses, epoch):\n","    \"\"\"\n","    Saves training and validation losses to an HDF5 file.\n","\n","    Arguments:\n","        training_losses: List of training losses\n","        validation_losses: List of validation losses\n","        filepath: Filepath to save the losses\n","    \"\"\"\n","    with h5py.File('/kaggle/working/losses_'+ str(epoch+1) + encoder_mode + '.h5', 'a') as hf:\n","        hf.create_dataset('training_losses', data=training_losses)\n","        hf.create_dataset('validation_losses', data=validation_losses)\n","    print(f\"Losses saved.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:46.315556Z","iopub.status.busy":"2025-03-09T11:05:46.315272Z","iopub.status.idle":"2025-03-09T11:05:46.320005Z","shell.execute_reply":"2025-03-09T11:05:46.319083Z","shell.execute_reply.started":"2025-03-09T11:05:46.315535Z"},"trusted":true},"outputs":[],"source":["# Load losses from an HDF5 file\n","def load_losses_h5(filepath):\n","    \"\"\"\n","    Loads training and validation losses from an HDF5 file.\n","\n","    Arguments:\n","        filepath: Filepath to load the losses\n","\n","    Returns:\n","        Training losses and validation losses as lists\n","    \"\"\"\n","    with h5py.File(filepath, 'r') as hf:\n","        training_losses = list(hf['training_losses'][:])\n","        validation_losses = list(hf['validation_losses'][:])\n","    print(f\"Losses loaded from {filepath}\")\n","    return training_losses, validation_losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:47.417032Z","iopub.status.busy":"2025-03-09T11:05:47.416657Z","iopub.status.idle":"2025-03-09T11:05:47.778101Z","shell.execute_reply":"2025-03-09T11:05:47.777144Z","shell.execute_reply.started":"2025-03-09T11:05:47.417004Z"},"trusted":true},"outputs":[],"source":["def plot_training_validation_graph(training_losses, validation_losses):\n","    \"\"\"\n","    Plots and saves training vs validation loss and top-5 accuracy over epochs.\n","\n","    Arguments:\n","        training_losses: List of training losses per epoch\n","        validation_losses: List of validation losses per epoch\n","        output_dir: Directory to save the plots\n","    \"\"\"\n","    epochs = range(1, len(training_losses) + 1)\n","    output_dir = '/kaggle/working/'\n","    plt.figure(figsize=(12, 5))\n","\n","    # Plot Loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, training_losses, label='Training Loss', marker='o')\n","    plt.plot(epochs, validation_losses, label='Validation Loss', marker='o')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Training vs Validation Loss')\n","    plt.legend()\n","\n","    # # Plot Top-5 Accuracy\n","    # plt.subplot(1, 2, 2)\n","    # plt.plot(epochs, training_top5, label='Training Top-5 Accuracy', marker='o')\n","    # plt.plot(epochs, validation_top5, label='Validation Top-5 Accuracy', marker='o')\n","    # plt.xlabel('Epochs')\n","    # plt.ylabel('Top-5 Accuracy')\n","    # plt.title('Training vs Validation Top-5 Accuracy')\n","    # plt.legend()\n","\n","    plt.tight_layout()\n","\n","    os.makedirs(output_dir, exist_ok=True)\n","    plot_path = os.path.join(output_dir, 'training_validation_plot.png')\n","    plt.savefig(plot_path)\n","    print(f\"Plot saved at {plot_path}\")\n","    plt.show()\n","\n","# Add lists to store the metrics\n","training_losses = []\n","validation_losses = []\n","\n","if losses_path is not None:\n","    loaded_training_losses, loaded_validation_losses = load_losses_h5(losses_path)\n","    training_losses = loaded_training_losses\n","    validation_losses = loaded_validation_losses\n","\n","\n","#To check the loaded losses\n","plot_training_validation_graph(training_losses, validation_losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:51.466899Z","iopub.status.busy":"2025-03-09T11:05:51.466510Z","iopub.status.idle":"2025-03-09T11:05:51.472148Z","shell.execute_reply":"2025-03-09T11:05:51.471194Z","shell.execute_reply.started":"2025-03-09T11:05:51.466868Z"},"trusted":true},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"\n","    Keeps track of most recent, average, sum, and count of a metric.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:52.404799Z","iopub.status.busy":"2025-03-09T11:05:52.404433Z","iopub.status.idle":"2025-03-09T11:05:52.409379Z","shell.execute_reply":"2025-03-09T11:05:52.408592Z","shell.execute_reply.started":"2025-03-09T11:05:52.404735Z"},"trusted":true},"outputs":[],"source":["def accuracy(scores, targets, k):\n","    \"\"\"\n","    Computes top-k accuracy, from predicted and true labels.\n","\n","    Arguments:\n","        scores: scores from the model\n","        targets: true labels\n","        k: k in top-k accuracy\n","    \n","    Returns:\n","        top-k accuracy\n","    \"\"\"\n","\n","    batch_size = targets.size(0)\n","    _, ind = scores.topk(k, 1, True, True)\n","    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n","    correct_total = correct.view(-1).float().sum()  # 0D tensor\n","    return correct_total.item() * (100.0 / batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:53.764738Z","iopub.status.busy":"2025-03-09T11:05:53.764423Z","iopub.status.idle":"2025-03-09T11:05:53.769267Z","shell.execute_reply":"2025-03-09T11:05:53.768317Z","shell.execute_reply.started":"2025-03-09T11:05:53.764711Z"},"trusted":true},"outputs":[],"source":["def adjust_learning_rate(optimizer, shrink_factor):\n","    \"\"\"\n","    Shrinks learning rate by a specified factor.\n","\n","    Arguments:\n","        optimizer: optimizer whose learning rate must be shrunk.\n","        shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n","    \"\"\"\n","\n","    print(\"\\nDECAYING learning rate.\")\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = param_group['lr'] * shrink_factor\n","    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:54.906712Z","iopub.status.busy":"2025-03-09T11:05:54.906399Z","iopub.status.idle":"2025-03-09T11:05:54.910981Z","shell.execute_reply":"2025-03-09T11:05:54.910122Z","shell.execute_reply.started":"2025-03-09T11:05:54.906688Z"},"trusted":true},"outputs":[],"source":["def lr_lambda(current_step):\n","    if current_step < num_warmup_steps:\n","        return float(current_step) / float(max(1, num_warmup_steps))\n","    return max(\n","        0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:05:56.761783Z","iopub.status.busy":"2025-03-09T11:05:56.761422Z","iopub.status.idle":"2025-03-09T11:05:56.766601Z","shell.execute_reply":"2025-03-09T11:05:56.765525Z","shell.execute_reply.started":"2025-03-09T11:05:56.761724Z"},"trusted":true},"outputs":[],"source":["def clip_gradient(optimizer, grad_clip):\n","    \"\"\"\n","    Clips gradients computed during backpropagation to avoid explosion of gradients.\n","\n","    Arguments:\n","        optimizer: optimizer with the gradients to be clipped\n","        grad_clip: clip value\n","    \"\"\"\n","    for group in optimizer.param_groups:\n","        for param in group['params']:\n","            if param.grad is not None:\n","                param.grad.data.clamp_(-grad_clip, grad_clip)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:01.347010Z","iopub.status.busy":"2025-03-09T11:06:01.346638Z","iopub.status.idle":"2025-03-09T11:06:01.352020Z","shell.execute_reply":"2025-03-09T11:06:01.351106Z","shell.execute_reply.started":"2025-03-09T11:06:01.346982Z"},"trusted":true},"outputs":[],"source":["def save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n","                    metrics, is_best, final_):\n","    \"\"\"\n","    Saves model checkpoint.\n","\n","    Arguments:\n","        data_name: base name of processed dataset\n","        epoch: epoch number\n","        epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n","        encoder: encoder model\n","        decoder: decoder model\n","        encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n","        decoder_optimizer: optimizer to update decoder's weights\n","        bleu4: validation BLEU-4 score for this epoch\n","        is_best: is this checkpoint the best so far?\n","    \"\"\"\n","    state = {'epoch': epoch,\n","             'epochs_since_improvement': epochs_since_improvement,\n","             'metrics': metrics,\n","             'encoder': encoder,\n","             'decoder': decoder,\n","             'encoder_optimizer': encoder_optimizer,\n","             'decoder_optimizer': decoder_optimizer,\n","             'final_': final_}\n","    filename = 'checkpoint_ep' + str(epoch+1) + '_b' + str(batch_size) + 'lr_' + str(decoder_lr) + encoder_mode + '.pth.tar'\n","    torch.save(state, filename)\n","    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n","    if is_best:\n","        torch.save(state, 'BEST_' + filename)"]},{"cell_type":"markdown","metadata":{},"source":["# BLEU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:04.298821Z","iopub.status.busy":"2025-03-09T11:06:04.298479Z","iopub.status.idle":"2025-03-09T11:06:04.303649Z","shell.execute_reply":"2025-03-09T11:06:04.302844Z","shell.execute_reply.started":"2025-03-09T11:06:04.298795Z"},"trusted":true},"outputs":[],"source":["# Copyright (c) 2004-2006 University of Maryland. All rights\n","# reserved. Do not redistribute without permission from the\n","# author. Not for commercial use.\n","\n","'''Provides:\n","cook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\n","cook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n","'''\n","\n","def precook(s, n=4, out=False):\n","    \"\"\"Takes a string as input and returns an object that can be given to\n","    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n","    can take string arguments as well.\"\"\"\n","    words = s.split()\n","    counts = defaultdict(int)\n","    for k in range(1,n+1):\n","        for i in range(len(words)-k+1):\n","            ngram = tuple(words[i:i+k])\n","            counts[ngram] += 1\n","    return (len(words), counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:09.838816Z","iopub.status.busy":"2025-03-09T11:06:09.838405Z","iopub.status.idle":"2025-03-09T11:06:09.844951Z","shell.execute_reply":"2025-03-09T11:06:09.843596Z","shell.execute_reply.started":"2025-03-09T11:06:09.838783Z"},"trusted":true},"outputs":[],"source":["def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n","    '''Takes a list of reference sentences for a single segment\n","    and returns an object that encapsulates everything that BLEU\n","    needs to know about them.'''\n","\n","    reflen = []\n","    maxcounts = {}\n","    for ref in refs:\n","        rl, counts = precook(ref, n)\n","        reflen.append(rl)\n","        for (ngram,count) in counts.items():\n","            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n","\n","    # Calculate effective reference sentence length.\n","    if eff == \"shortest\":\n","        reflen = min(reflen)\n","    elif eff == \"average\":\n","        reflen = float(sum(reflen))/len(reflen)\n","\n","    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n","    \n","    ## lhuang: N.B.: in case of \"closest\", keep a list of reflens!! (bad design)\n","\n","    return (reflen, maxcounts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:09.863078Z","iopub.status.busy":"2025-03-09T11:06:09.862611Z","iopub.status.idle":"2025-03-09T11:06:09.869036Z","shell.execute_reply":"2025-03-09T11:06:09.868140Z","shell.execute_reply.started":"2025-03-09T11:06:09.863050Z"},"trusted":true},"outputs":[],"source":["def cook_test(test, xxx_todo_changeme, eff=None, n=4):\n","    '''Takes a test sentence and returns an object that\n","    encapsulates everything that BLEU needs to know about it.'''\n","    (reflen, refmaxcounts) = xxx_todo_changeme\n","    testlen, counts = precook(test, n, True)\n","\n","    result = {}\n","\n","    # Calculate effective reference sentence length.\n","    \n","    if eff == \"closest\":\n","        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n","    else: ## i.e., \"average\" or \"shortest\" or None\n","        result[\"reflen\"] = reflen\n","\n","    result[\"testlen\"] = testlen\n","\n","    result[\"guess\"] = [max(0,testlen-k+1) for k in range(1,n+1)]\n","\n","    result['correct'] = [0]*n\n","    for (ngram, count) in counts.items():\n","        result[\"correct\"][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n","\n","    return result\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:09.893769Z","iopub.status.busy":"2025-03-09T11:06:09.893487Z","iopub.status.idle":"2025-03-09T11:06:09.913018Z","shell.execute_reply":"2025-03-09T11:06:09.912215Z","shell.execute_reply.started":"2025-03-09T11:06:09.893726Z"},"trusted":true},"outputs":[],"source":["class BleuScorer(object):\n","    \"\"\"\n","    Bleu scorer.\n","    \"\"\"\n","\n","    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n","    # special_reflen is used in oracle (proportional effective ref len for a node).\n","\n","    def copy(self):\n","        ''' copy the refs.'''\n","        new = BleuScorer(n=self.n)\n","        new.ctest = copy.copy(self.ctest)\n","        new.crefs = copy.copy(self.crefs)\n","        new._score = None\n","        return new\n","\n","    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n","        ''' singular instance '''\n","\n","        self.n = n\n","        self.crefs = []\n","        self.ctest = []\n","        self.cook_append(test, refs)\n","        self.special_reflen = special_reflen\n","\n","    def cook_append(self, test, refs):\n","        '''called by constructor and __iadd__ to avoid creating new instances.'''\n","        \n","        if refs is not None:\n","            self.crefs.append(cook_refs(refs))\n","            if test is not None:\n","                cooked_test = cook_test(test, self.crefs[-1])\n","                self.ctest.append(cooked_test) ## N.B.: -1\n","            else:\n","                self.ctest.append(None) # lens of crefs and ctest have to match\n","\n","        self._score = None ## need to recompute\n","\n","    def ratio(self, option=None):\n","        self.compute_score(option=option)\n","        return self._ratio\n","\n","    def score_ratio(self, option=None):\n","        '''return (bleu, len_ratio) pair'''\n","        return (self.fscore(option=option), self.ratio(option=option))\n","\n","    def score_ratio_str(self, option=None):\n","        return \"%.4f (%.2f)\" % self.score_ratio(option)\n","\n","    def reflen(self, option=None):\n","        self.compute_score(option=option)\n","        return self._reflen\n","\n","    def testlen(self, option=None):\n","        self.compute_score(option=option)\n","        return self._testlen        \n","\n","    def retest(self, new_test):\n","        if type(new_test) is str:\n","            new_test = [new_test]\n","        assert len(new_test) == len(self.crefs), new_test\n","        self.ctest = []\n","        for t, rs in zip(new_test, self.crefs):\n","            self.ctest.append(cook_test(t, rs))\n","        self._score = None\n","\n","        return self\n","\n","    def rescore(self, new_test):\n","        ''' replace test(s) with new test(s), and returns the new score.'''\n","        \n","        return self.retest(new_test).compute_score()\n","\n","    def size(self):\n","        assert len(self.crefs) == len(self.ctest), \"refs/test mismatch! %d<>%d\" % (len(self.crefs), len(self.ctest))\n","        return len(self.crefs)\n","\n","    def __iadd__(self, other):\n","        '''add an instance (e.g., from another sentence).'''\n","\n","        if type(other) is tuple:\n","            ## avoid creating new BleuScorer instances\n","            self.cook_append(other[0], other[1])\n","        else:\n","            assert self.compatible(other), \"incompatible BLEUs.\"\n","            self.ctest.extend(other.ctest)\n","            self.crefs.extend(other.crefs)\n","            self._score = None ## need to recompute\n","\n","        return self        \n","\n","    def compatible(self, other):\n","        return isinstance(other, BleuScorer) and self.n == other.n\n","\n","    def single_reflen(self, option=\"average\"):\n","        return self._single_reflen(self.crefs[0][0], option)\n","\n","    def _single_reflen(self, reflens, option=None, testlen=None):\n","        \n","        if option == \"shortest\":\n","            reflen = min(reflens)\n","        elif option == \"average\":\n","            reflen = float(sum(reflens))/len(reflens)\n","        elif option == \"closest\":\n","            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n","        else:\n","            assert False, \"unsupported reflen option %s\" % option\n","\n","        return reflen\n","\n","    def recompute_score(self, option=None, verbose=0):\n","        self._score = None\n","        return self.compute_score(option, verbose)\n","        \n","    def compute_score(self, option=None, verbose=0):\n","        n = self.n\n","        small = 1e-9\n","        tiny = 1e-15 ## so that if guess is 0 still return 0\n","        bleu_list = [[] for _ in range(n)]\n","\n","        if self._score is not None:\n","            return self._score\n","\n","        if option is None:\n","            option = \"average\" if len(self.crefs) == 1 else \"closest\"\n","\n","        self._testlen = 0\n","        self._reflen = 0\n","        totalcomps = {'testlen':0, 'reflen':0, 'guess':[0]*n, 'correct':[0]*n}\n","\n","        # for each sentence\n","        for comps in self.ctest:            \n","            testlen = comps['testlen']\n","            self._testlen += testlen\n","\n","            if self.special_reflen is None: ## need computation\n","                reflen = self._single_reflen(comps['reflen'], option, testlen)\n","            else:\n","                reflen = self.special_reflen\n","\n","            self._reflen += reflen\n","                \n","            for key in ['guess','correct']:\n","                for k in range(n):\n","                    totalcomps[key][k] += comps[key][k]\n","\n","            # append per image bleu score\n","            bleu = 1.\n","            for k in range(n):\n","                bleu *= (float(comps['correct'][k]) + tiny) \\\n","                        /(float(comps['guess'][k]) + small) \n","                bleu_list[k].append(bleu ** (1./(k+1)))\n","            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n","            if ratio < 1:\n","                for k in range(n):\n","                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n","\n","            # if verbose > 1:\n","            #     print(comps, reflen)\n","\n","        totalcomps['reflen'] = self._reflen\n","        totalcomps['testlen'] = self._testlen\n","\n","        bleus = []\n","        bleu = 1.\n","        for k in range(n):\n","            bleu *= float(totalcomps['correct'][k] + tiny) \\\n","                    / (totalcomps['guess'][k] + small)\n","            bleus.append(bleu ** (1./(k+1)))\n","        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n","        if ratio < 1:\n","            for k in range(n):\n","                bleus[k] *= math.exp(1 - 1/ratio)\n","\n","        # if verbose > 0:\n","        #     print(totalcomps)\n","        #     print(\"ratio:\", ratio)\n","\n","        self._score = bleus\n","        return self._score, bleu_list"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:11.771253Z","iopub.status.busy":"2025-03-09T11:06:11.770942Z","iopub.status.idle":"2025-03-09T11:06:11.777244Z","shell.execute_reply":"2025-03-09T11:06:11.776282Z","shell.execute_reply.started":"2025-03-09T11:06:11.771232Z"},"trusted":true},"outputs":[],"source":["class Bleu:\n","    def __init__(self, n=4):\n","        # default compute Blue score up to 4\n","        self._n = n\n","        self._hypo_for_image = {}\n","        self.ref_for_image = {}\n","\n","    def compute_score(self, gts, res):\n","\n","        bleu_scorer = BleuScorer(n=self._n)\n","        for i in range(len(res)):\n","            hypo = res[i]\n","            ref = gts[i]\n","\n","            # Sanity check.\n","            assert(type(hypo) is list)\n","            assert(len(hypo) == 1)\n","            assert(type(ref) is list)\n","            assert(len(ref) >= 1)\n","\n","            bleu_scorer += (hypo[0], ref)\n","\n","        #score, scores = bleu_scorer.compute_score(option='shortest')\n","        score, scores = bleu_scorer.compute_score(option='closest', verbose=1)\n","        #score, scores = bleu_scorer.compute_score(option='average', verbose=1)\n","\n","        # return (bleu, bleu_info)\n","        return score, scores\n","\n","    def method(self):\n","        return \"Bleu\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:12.228223Z","iopub.status.busy":"2025-03-09T11:06:12.227939Z","iopub.status.idle":"2025-03-09T11:06:12.234137Z","shell.execute_reply":"2025-03-09T11:06:12.233243Z","shell.execute_reply.started":"2025-03-09T11:06:12.228202Z"},"trusted":true},"outputs":[],"source":["def get_eval_score(references, hypotheses):\n","    scorers = [\n","        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"])\n","    ]\n","\n","    hypo = [[' '.join(hypo)] for hypo in [[str(x) for x in hypo] for hypo in hypotheses]]\n","    ref = [[' '.join(reft) for reft in reftmp] for reftmp in\n","           [[[str(x) for x in reft] for reft in reftmp] for reftmp in references]]\n","\n","    score = []\n","    method = []\n","    for scorer, method_i in scorers:\n","        score_i, scores_i = scorer.compute_score(ref, hypo)\n","        score.extend(score_i) if isinstance(score_i, list) else score.append(score_i)\n","        method.extend(method_i) if isinstance(method_i, list) else method.append(method_i)\n","        print(\"{} {}\".format(method_i, score_i))\n","    score_dict = dict(zip(method, score))\n","\n","    return score_dict"]},{"cell_type":"markdown","metadata":{},"source":["# TRAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:13.707256Z","iopub.status.busy":"2025-03-09T11:06:13.706969Z","iopub.status.idle":"2025-03-09T11:06:13.717399Z","shell.execute_reply":"2025-03-09T11:06:13.716463Z","shell.execute_reply.started":"2025-03-09T11:06:13.707236Z"},"trusted":true},"outputs":[],"source":["def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n","    \"\"\"\n","    Performs one epoch's training.\n","\n","    Arguments:\n","        train_loader: DataLoader for training data\n","        encoder: encoder model\n","        decoder: decoder model\n","        criterion: loss layer\n","        encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n","        decoder_optimizer: optimizer to update decoder's weights\n","        epoch: epoch number\n","    \"\"\"\n","\n","    decoder.train()  # train mode (dropout and batchnorm is used)\n","    encoder.train()\n","\n","    batch_time = AverageMeter()  # forward prop. + back prop. time\n","    data_time = AverageMeter()  # data loading time\n","    losses = AverageMeter()  # loss (per word decoded)\n","    top5accs = AverageMeter()  # top5 accuracy\n","\n","    start = time.time()\n","    scheduler = LambdaLR(decoder_optimizer, lr_lambda)\n","\n","    # Batches\n","    for i, (imgs, caps, caplens) in enumerate(train_loader):\n","        data_time.update(time.time() - start)\n","\n","        # Move to GPU, if available\n","        imgs = imgs.to(device)\n","        caps = caps.to(device)\n","        caplens = caplens.to(device)\n","\n","        # Forward prop.\n","        imgs = encoder(imgs)\n","        # imgs: [batch_size, 14, 14, 2048]\n","        # caps: [batch_size, 52]\n","        # caplens: [batch_size, 1]\n","        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n","        sort_ind = sort_ind.to(device)\n","\n","        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n","        targets = caps_sorted[:, 1:]\n","\n","        # Remove timesteps that we didn't decode at, or are pads\n","        # pack_padded_sequence is an easy trick to do this\n","        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n","        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n","        # print(scores.size())\n","        # print(targets.size())\n","\n","        # Calculate loss\n","        loss = criterion(scores, targets)\n","        # Add doubly stochastic attention regularization\n","        # Second loss, mentioned in paper \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\"\n","        # https://arxiv.org/abs/1502.03044\n","        # In section 4.2.1 Doubly stochastic attention regularization: We know the weights sum to 1 at a given timestep.\n","        # But we also encourage the weights at a single pixel p to sum to 1 across all timesteps T.\n","        # This means we want the model to attend to every pixel over the course of generating the entire sequence.\n","        # Therefore, we want to minimize the difference between 1 and the sum of a pixel's weights across all timesteps.\n","        \n","     \n","        dec_alphas = alphas[\"dec_enc_attns\"]\n","        alpha_trans_c = alpha_c / (n_heads * decoder_layers)\n","        for layer in range(decoder_layers):  # decoder_layers = len(dec_alphas)\n","            cur_layer_alphas = dec_alphas[layer]  # [batch_size, n_heads, 52, 196]\n","            for h in range(n_heads):\n","                cur_head_alpha = cur_layer_alphas[:, h, :, :]\n","                loss += alpha_trans_c * ((1. - cur_head_alpha.sum(dim=1)) ** 2).mean()\n","\n","        # Back prop.\n","        decoder_optimizer.zero_grad()\n","        if encoder_optimizer is not None:\n","            encoder_optimizer.zero_grad()\n","        loss.backward()\n","\n","        # Clip gradients\n","        if grad_clip is not None:\n","            clip_gradient(decoder_optimizer, grad_clip)\n","            if encoder_optimizer is not None:\n","                clip_gradient(encoder_optimizer, grad_clip)\n","\n","        # Update weights\n","        decoder_optimizer.step()\n","        scheduler.step()\n","        if encoder_optimizer is not None:\n","            encoder_optimizer.step()\n","\n","\n","        # Keep track of metrics\n","        top5 = accuracy(scores, targets, 5)\n","        losses.update(loss.item(), sum(decode_lengths))\n","        top5accs.update(top5, sum(decode_lengths))\n","        batch_time.update(time.time() - start)\n","        start = time.time()\n","        if i % print_freq == 0:\n","            print(\"Epoch: {}/{} step: {}/{} Loss: {} AVG_Loss: {} Top-5 Accuracy: {} Batch_time: {}s\".format(epoch+1, epochs, i+1, len(train_loader), losses.val, losses.avg, top5accs.avg, batch_time.val))\n","        \n","    return losses.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:16.051720Z","iopub.status.busy":"2025-03-09T11:06:16.051384Z","iopub.status.idle":"2025-03-09T11:06:16.063404Z","shell.execute_reply":"2025-03-09T11:06:16.062457Z","shell.execute_reply.started":"2025-03-09T11:06:16.051693Z"},"trusted":true},"outputs":[],"source":["def validate(val_loader, encoder, decoder, criterion):\n","    \"\"\"\n","    Performs one epoch's validation.\n","\n","    Arguments:\n","        val_loader: DataLoader for validation data.\n","        encoder: encoder model\n","        decoder: decoder model\n","        criterion: loss layer\n","\n","    Returns:\n","        score_dict {'Bleu_1': 0., 'Bleu_2': 0., 'Bleu_3': 0., 'Bleu_4': 0.}\n","    \"\"\"\n","    decoder.eval()  # eval mode (no dropout or batchnorm)\n","    if encoder is not None:\n","        encoder.eval()\n","\n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    top5accs = AverageMeter()\n","\n","    start = time.time()\n","\n","    references = list()  # references (true captions) for calculating BLEU-4 score\n","    hypotheses = list()  # hypotheses (predictions)\n","\n","    # explicitly disable gradient calculation to avoid CUDA memory error\n","    with torch.no_grad():\n","        # Batches\n","        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n","\n","            # Move to device, if available\n","            imgs = imgs.to(device)\n","            caps = caps.to(device)\n","            caplens = caplens.to(device)\n","            allcaps = allcaps.to(device)\n","\n","            # Forward prop.\n","            if encoder is not None:\n","                imgs = encoder(imgs)\n","            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n","            sort_ind = sort_ind.to(device)\n","\n","            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n","            targets = caps_sorted[:, 1:]\n","\n","            # Remove timesteps that we didn't decode at, or are pads\n","            # pack_padded_sequence is an easy trick to do this\n","            scores_copy = scores.clone()\n","            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n","            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n","\n","            # Calculate loss\n","            loss = criterion(scores, targets)\n","\n","            # Add doubly stochastic attention regularization\n","            \n","       \n","            dec_alphas = alphas[\"dec_enc_attns\"]\n","            alpha_trans_c = alpha_c / (n_heads * decoder_layers)\n","            for layer in range(decoder_layers):  # decoder_layers = len(dec_alphas)\n","                cur_layer_alphas = dec_alphas[layer]  # [batch_size, n_heads, 52, 196]\n","                for h in range(n_heads):\n","                    cur_head_alpha = cur_layer_alphas[:, h, :, :]\n","                    loss += alpha_trans_c * ((1. - cur_head_alpha.sum(dim=1)) ** 2).mean()\n","\n","            # Keep track of metrics\n","            losses.update(loss.item(), sum(decode_lengths))\n","            top5 = accuracy(scores, targets, 5)\n","            top5accs.update(top5, sum(decode_lengths))\n","            batch_time.update(time.time() - start)\n","            start = time.time()\n","\n","            # Store references (true captions), and hypothesis (prediction) for each image\n","            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n","            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n","\n","            # References\n","            allcaps = allcaps[sort_ind] # because images were sorted in the decoder\n","            for j in range(allcaps.shape[0]):\n","                img_caps = allcaps[j].tolist()\n","                img_captions = list(\n","                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n","                        img_caps))  # remove <start> and pads\n","                references.append(img_captions)\n","\n","            # Hypotheses\n","            _, preds = torch.max(scores_copy, dim=2)\n","            preds = preds.tolist()\n","            temp_preds = list()\n","            for j, p in enumerate(preds):\n","                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n","            preds = temp_preds\n","            hypotheses.extend(preds)\n","\n","            assert len(references) == len(hypotheses)\n","\n","    # Calculate BLEU-1~4 scores\n","    # metrics = {}\n","    # weights = (1.0 / 1.0,)\n","    # metrics[\"bleu1\"] = corpus_bleu(references, hypotheses, weights)\n","    # weights = (1.0/2.0, 1.0/2.0,)\n","    # metrics[\"bleu2\"] = corpus_bleu(references, hypotheses, weights)\n","    # weights = (1.0 / 3.0, 1.0 / 3.0, 1.0 / 3.0,)\n","    # metrics[\"bleu3\"] = corpus_bleu(references, hypotheses, weights)\n","    # metrics[\"bleu4\"] = corpus_bleu(references, hypotheses)\n","\n","    # Calculate BLEU1~4, METEOR, ROUGE_L, CIDEr scores\n","    metrics = get_eval_score(references, hypotheses)\n","\n","    print(\"EVA LOSS: {} TOP-5 Accuracy {} BLEU-1 {} BLEU2 {} BLEU3 {} BLEU-4 {}\".format\n","          (losses.avg, top5accs.avg,  metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n","\n","    validation_losses.append(losses.avg)\n","    return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:18.630594Z","iopub.status.busy":"2025-03-09T11:06:18.630181Z","iopub.status.idle":"2025-03-09T11:06:18.636497Z","shell.execute_reply":"2025-03-09T11:06:18.635496Z","shell.execute_reply.started":"2025-03-09T11:06:18.630561Z"},"trusted":true},"outputs":[],"source":["# load checkpoint, these parameters can't be modified\n","final_args = {\"emb_dim\": emb_dim,\n","             \"attention_dim\": attention_dim,\n","             \"decoder_dim\": decoder_dim,\n","             \"n_heads\": n_heads,\n","             \"dropout\": dropout,\n","             \"decoder_mode\": decoder_mode,\n","             \"encoder_layers\": encoder_layers,\n","             \"decoder_layers\": decoder_layers}\n","\n","start_epoch = 0\n","best_bleu4 = 0.  # BLEU-4 score right now\n","epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n","\n","cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n","print(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:19.924997Z","iopub.status.busy":"2025-03-09T11:06:19.924655Z","iopub.status.idle":"2025-03-09T11:06:19.948550Z","shell.execute_reply":"2025-03-09T11:06:19.947654Z","shell.execute_reply.started":"2025-03-09T11:06:19.924974Z"},"trusted":true},"outputs":[],"source":["# Read word map\n","word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n","with open(word_map_file, 'r') as j:\n","    word_map = json.load(j)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:06:21.057840Z","iopub.status.busy":"2025-03-09T11:06:21.057517Z","iopub.status.idle":"2025-03-09T11:07:15.686774Z","shell.execute_reply":"2025-03-09T11:07:15.685811Z","shell.execute_reply.started":"2025-03-09T11:06:21.057814Z"},"trusted":true},"outputs":[],"source":["# Initialize / load checkpoint\n","if checkpoint is None:\n","    encoder = CNN_Encoder()\n","    encoder.fine_tune(fine_tune_encoder)\n","    encoder_optimizer = torch.optim.AdamW(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                         lr=encoder_lr, weight_decay=1e-4) if fine_tune_encoder else None\n","\n","    \n","    decoder = Transformer(vocab_size=len(word_map), embed_dim=emb_dim, encoder_layers=encoder_layers,\n","                              decoder_layers=decoder_layers, dropout=dropout, n_heads=n_heads)\n","\n","    decoder_optimizer = torch.optim.AdamW(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n","                                         lr=decoder_lr, weight_decay=1e-4)\n","\n","    # load pre-trained word embedding\n","    if embedding_path is not None:\n","        all_word_embeds = {}\n","        for i, line in enumerate(codecs.open(embedding_path, 'r', 'utf-8')):\n","            s = line.strip().split()\n","            all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n","\n","        # change emb_dim\n","        emb_dim = list(all_word_embeds.values())[-1].size\n","        word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_map), emb_dim))\n","        for w in word_map:\n","            if w in all_word_embeds:\n","                word_embeds[word_map[w]] = all_word_embeds[w]\n","            elif w.lower() in all_word_embeds:\n","                word_embeds[word_map[w]] = all_word_embeds[w.lower()]\n","            else:\n","                # <pad> <start> <end> <unk>\n","                embedding_i = torch.ones(1, emb_dim)\n","                torch.nn.init.xavier_uniform_(embedding_i)\n","                word_embeds[word_map[w]] = embedding_i\n","\n","        word_embeds = torch.FloatTensor(word_embeds).to(device)\n","        decoder.load_pretrained_embeddings(word_embeds)\n","        decoder.fine_tune_embeddings(fine_tune_embedding)\n","        print('Loaded {} pre-trained word embeddings.'.format(len(word_embeds)))\n","\n","else:\n","    checkpoint = torch.load(checkpoint, map_location=str(device))\n","    start_epoch = checkpoint['epoch'] + 1\n","    epochs_since_improvement = checkpoint['epochs_since_improvement']\n","    best_bleu4 = checkpoint['metrics'][\"Bleu_4\"]\n","    encoder = checkpoint['encoder']\n","    encoder_optimizer = checkpoint['encoder_optimizer']\n","    decoder = checkpoint['decoder']\n","    decoder_optimizer = checkpoint['decoder_optimizer']\n","    decoder.fine_tune_embeddings(fine_tune_embedding)\n","    # load final_args from checkpoint\n","    \n","    if fine_tune_encoder is True and encoder_optimizer is None:\n","        print(\"Encoder_Optimizer is None, Creating new Encoder_Optimizer!\")\n","        encoder.fine_tune(fine_tune_encoder)\n","        encoder_optimizer = torch.optim.W(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n","                                             lr=encoder_lr, weight_decay=1e-4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:09:17.891427Z","iopub.status.busy":"2025-03-09T11:09:17.891123Z","iopub.status.idle":"2025-03-09T11:09:17.964425Z","shell.execute_reply":"2025-03-09T11:09:17.963458Z","shell.execute_reply.started":"2025-03-09T11:09:17.891405Z"},"trusted":true},"outputs":[],"source":["# Move to GPU, if available\n","decoder = decoder.to(device)\n","encoder = encoder.to(device)\n","print(\"encoder_layers {} decoder_layers {} n_heads {} dropout {} encoder_lr {} \"\n","      \"decoder_lr {} alpha_c {}\".format(encoder_layers, decoder_layers, n_heads, dropout,\n","                                        encoder_lr, decoder_lr, alpha_c))\n","#print(encoder)\n","#print(decoder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:09:19.893854Z","iopub.status.busy":"2025-03-09T11:09:19.893478Z","iopub.status.idle":"2025-03-09T11:09:19.897881Z","shell.execute_reply":"2025-03-09T11:09:19.896943Z","shell.execute_reply.started":"2025-03-09T11:09:19.893821Z"},"trusted":true},"outputs":[],"source":["# Loss function\n","criterion = nn.CrossEntropyLoss().to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:09:21.019583Z","iopub.status.busy":"2025-03-09T11:09:21.019245Z","iopub.status.idle":"2025-03-09T11:09:25.780824Z","shell.execute_reply":"2025-03-09T11:09:25.779964Z","shell.execute_reply.started":"2025-03-09T11:09:21.019555Z"},"trusted":true},"outputs":[],"source":["# Custom dataloaders\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","# normalize = transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","\n","# pin_memory: If True, the data loader will copy Tensors into CUDA pinned memory before returning them.\n","# If your data elements are a custom type, or your collate_fn returns a batch that is a custom type.\n","train_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n","num_training_steps = len(train_loader)\n","num_warmup_steps = num_training_steps * 0.1\n","print(\"Warmup for {} steps.\".format(num_warmup_steps))\n","val_loader = torch.utils.data.DataLoader(\n","    CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n","    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T11:09:31.211052Z","iopub.status.busy":"2025-03-09T11:09:31.210728Z","iopub.status.idle":"2025-03-09T14:28:25.889040Z","shell.execute_reply":"2025-03-09T14:28:25.887758Z","shell.execute_reply.started":"2025-03-09T11:09:31.211029Z"},"trusted":true},"outputs":[],"source":["# Epochs\n","for epoch in range(start_epoch, epochs):\n","\n","    # Decay learning rate if there is no improvement for 5 consecutive epochs, and terminate training after 25\n","    # 8 20\n","    if epochs_since_improvement == stop_criteria:\n","        print(\"the model has not improved in the last {} epochs\".format(stop_criteria))\n","        break\n","    if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n","        adjust_learning_rate(decoder_optimizer, 0.8)\n","        if fine_tune_encoder and encoder_optimizer is not None:\n","            print(encoder_optimizer)\n","            adjust_learning_rate(encoder_optimizer, 0.8)\n","\n","    # One epoch's training\n","    train_loss = train(train_loader=train_loader, encoder=encoder, decoder=decoder, criterion=criterion, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, epoch=epoch)\n","\n","    # One epoch's validation\n","    metrics = validate(val_loader=val_loader, encoder=encoder, decoder=decoder, criterion=criterion)\n","    recent_bleu4 = metrics[\"Bleu_4\"]\n","\n","    # Check if there was an improvement\n","    is_best = recent_bleu4 > best_bleu4\n","    best_bleu4 = max(recent_bleu4, best_bleu4)\n","    if not is_best:\n","        epochs_since_improvement += 1\n","        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n","    else:\n","        epochs_since_improvement = 0\n","\n","    # Save checkpoint\n","    save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n","                    decoder_optimizer, metrics, is_best, final_args)\n","\n","    training_losses.append(train_loss)\n","    save_losses_h5(training_losses, validation_losses, epoch)\n","    plot_training_validation_graph(training_losses, validation_losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["validation_losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T10:46:14.297776Z","iopub.status.busy":"2025-03-09T10:46:14.297446Z","iopub.status.idle":"2025-03-09T10:46:14.303134Z","shell.execute_reply":"2025-03-09T10:46:14.302175Z","shell.execute_reply.started":"2025-03-09T10:46:14.297728Z"},"trusted":true},"outputs":[],"source":["training_losses"]},{"cell_type":"markdown","metadata":{},"source":["# CAPTIONING"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T10:55:34.240274Z","iopub.status.busy":"2025-03-09T10:55:34.239961Z","iopub.status.idle":"2025-03-09T10:55:34.253390Z","shell.execute_reply":"2025-03-09T10:55:34.252566Z","shell.execute_reply.started":"2025-03-09T10:55:34.240250Z"},"trusted":true},"outputs":[],"source":["def caption_image_beam_search(encoder, decoder, img, word_map, beam_size):\n","    \"\"\"\n","    Reads an image and captions it with beam search.\n","\n","    Arguments:\n","        encoder: encoder model\n","        decoder: decoder model\n","        image_path: path to image\n","        word_map: word map\n","        beam_size: number of sequences to consider at each decode-step\n","    \n","    Returns:\n","        caption\n","    \"\"\"\n","\n","    k = beam_size\n","    Caption_End = False\n","    vocab_size = len(word_map)\n","\n","    # Read image and process\n","    img = Image.open(img)\n","    # Convert any image with more than 3(rgb) channels to RGB\n","    img = img.convert('RGB')\n","    img = np.array(img)\n","\n","    if len(img.shape) == 2:\n","        img = img[:, :, np.newaxis]\n","        img = np.concatenate([img, img, img], axis=2)\n","    img = np.array(Image.fromarray(img).resize((256, 256)))\n","    # img = imresize(img, (256, 256))\n","    img = img.transpose(2, 0, 1)\n","    img = img / 255.\n","    img = torch.FloatTensor(img).to(device)\n","    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","    transform = transforms.Compose([normalize])\n","    image = transform(img)  # (3, 256, 256)\n","\n","    # Encode\n","    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n","    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n","    enc_image_size = encoder_out.size(1)\n","    encoder_dim = encoder_out.size(-1)\n","    # Flatten encoding\n","    encoder_out = encoder_out.view(1, -1, encoder_dim)  # [1, num_pixels=196, encoder_dim]\n","    num_pixels = encoder_out.size(1)\n","    # We'll treat the problem as having a batch size of k\n","    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n","\n","    # Tensor to store top k previous words at each step; now they're just <start>\n","    k_prev_words = torch.LongTensor([[word_map['<start>']] * 52] * k).to(device)  # (k, 52)\n","\n","    # Tensor to store top k sequences; now they're just <start>\n","    seqs = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n","    # Tensor to store top k sequences' scores; now they're just 0\n","    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n","    # Tensor to store top k sequences' alphas; now they're just 1s\n","    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n","    # Lists to store completed sequences, their alphas and scores\n","    complete_seqs = list()\n","    complete_seqs_alpha = list()\n","    complete_seqs_scores = list()\n","\n","    # Start decoding\n","    step = 1\n","\n","    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n","    while True:\n","        cap_len = torch.LongTensor([52]).repeat(k, 1)  # [s, 1]\n","        scores, _, _, alpha_dict, _ = decoder(encoder_out, k_prev_words, cap_len)\n","        scores = scores[:, step - 1, :].squeeze(1)  # [s, 1, vocab_size] -> [s, vocab_size]\n","        # choose the last layer, transformer decoder is comosed of a stack of 6 identical layers.\n","        alpha = alpha_dict[\"dec_enc_attns\"][-1]  # [s, n_heads=8, len_q=52, len_k=196]\n","        # TODO: AVG Attention to Visualize\n","        # for i in range(len(alpha_dict[\"dec_enc_attns\"])):\n","        #     n_heads = alpha_dict[\"dec_enc_attns\"][i].size(1)\n","        #     for j in range(n_heads):\n","        #         pass\n","        # the second dim corresponds to the Multi-head attention = 8, now 0\n","        # the third dim corresponds to cur caption position\n","        alpha = alpha[:, 0, step-1, :].view(k, 1, enc_image_size, enc_image_size)  # [s, 1, enc_image_size, enc_image_size]\n","\n","        scores = F.log_softmax(scores, dim=1)\n","        # Add\n","        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n","        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n","        if step == 1:\n","            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n","        else:\n","            # Unroll and find top scores, and their unrolled indices\n","            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n","\n","        # Convert unrolled indices to actual indices of scores\n","        prev_word_inds = top_k_words // vocab_size  # (s)\n","        next_word_inds = top_k_words % vocab_size  # (s)\n","        # Add new words to sequences, alphas\n","        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n","        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds]], dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n","\n","        # Which sequences are incomplete (didn't reach <end>)?\n","        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n","                           next_word != word_map['<end>']]\n","        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n","        # Set aside complete sequences\n","        if len(complete_inds) > 0:\n","            Caption_End = True\n","            complete_seqs.extend(seqs[complete_inds].tolist())\n","            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n","            complete_seqs_scores.extend(top_k_scores[complete_inds])\n","        k -= len(complete_inds)  # reduce beam length accordingly\n","\n","        # Proceed with incomplete sequences\n","        if k == 0:\n","            break\n","        seqs = seqs[incomplete_inds]\n","        seqs_alpha = seqs_alpha[incomplete_inds]\n","        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n","        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n","        \n","        k_prev_words = k_prev_words[incomplete_inds]\n","        k_prev_words[:, :step + 1] = seqs  # [s, 52]\n","        # k_prev_words[:, step] = next_word_inds[incomplete_inds]  # [s, 52]\n","\n","        # Break if things have been going on too long\n","        if step > 50:\n","            break\n","        step += 1\n","\n","    assert Caption_End\n","    i = complete_seqs_scores.index(max(complete_seqs_scores))\n","    seq = complete_seqs[i]\n","    alphas = complete_seqs_alpha[i]\n","\n","    return seq\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-03-09T10:55:34.807243Z","iopub.status.busy":"2025-03-09T10:55:34.806938Z","iopub.status.idle":"2025-03-09T10:55:36.921003Z","shell.execute_reply":"2025-03-09T10:55:36.920075Z","shell.execute_reply.started":"2025-03-09T10:55:34.807221Z"},"trusted":true},"outputs":[],"source":["img = \"/kaggle/input/icrt-coco/dataset/val2014/COCO_val2014_000000000474.jpg\" # path to image, file or folder\n","checkpoint = \"/kaggle/working/BEST_checkpoint_ep1_b32lr_0.0001_rn101_.pth.tar\" # path to model\n","word_map = \"/kaggle/input/icrt-coco/dataset/generated_data/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json\" # path to word map JSON'\n","save_img_dir = \"/kaggle/working/\" # path to save annotated img\n","beam_size = 3 # beam size for beam search\n","# dont_smooth ='store_false' # do not smooth alpha overlay\n","\n","# Load model\n","checkpoint = torch.load(checkpoint, map_location=str(device), weights_only=False)\n","decoder = checkpoint['decoder']\n","decoder = decoder.to(device)\n","decoder.eval()\n","encoder = checkpoint['encoder']\n","encoder = encoder.to(device)\n","encoder.eval()\n","# print(encoder)\n","# print(decoder)\n","\n","# Load word map (word2ix)\n","with open(word_map, 'r') as j:\n","    word_map = json.load(j)\n","    \n","rev_word_map = {v: k for k, v in word_map.items()}  # idx to word\n","\n","    \n","with torch.no_grad():\n","    seq = caption_image_beam_search(encoder, decoder, img, word_map, beam_size)\n","    \n","words = [rev_word_map[ind] for ind in seq]\n","words = words[1:-1]\n","\n","print(\" \".join(words))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6353203,"sourceId":10268740,"sourceType":"datasetVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.9.21 ('nlp')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"},"vscode":{"interpreter":{"hash":"dea56c0b954ae67294c7ca9bf9c057fc748d59d22faedcf3e8a57e0a70ba84c3"}}},"nbformat":4,"nbformat_minor":4}
